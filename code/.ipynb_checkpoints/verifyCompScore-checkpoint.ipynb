{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proxyLabelAssignement import proxyLabelAssignement\n",
    "import collections\n",
    "import ast\n",
    "import re\n",
    "\n",
    "out_filename = 'output/compatibility_with_proxyLabel.txt'\n",
    "out_filename_ml = 'output/compatibility_with_proxyLabel_ML.txt'\n",
    "\n",
    "infile = open('../dataset/users_evaluation.json','r')\n",
    "dictionary = ast.literal_eval(infile.readline())\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infile = open('output/compatibility.json','r')\n",
    "users = ast.literal_eval(infile.readline())\n",
    "infile.close()\n",
    "infile = open('output/compatibility_ml.json','r')\n",
    "users_ml = ast.literal_eval(infile.readline())\n",
    "infile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compScore = {}\n",
    "compScore_ml = {}\n",
    "for user in users:\n",
    "    compScore[user] = {}\n",
    "    user_X = dictionary[(user)]\n",
    "    for compUser in users[user]:\n",
    "        user_Y = dictionary[compUser]\n",
    "        score = proxyLabelAssignement(user_X, user_Y)\n",
    "        if score >= 0.5:\n",
    "            compScore[user][compUser] = 'Yes'\n",
    "        else:\n",
    "            compScore[user][compUser] = 'No'          \n",
    "\n",
    "for user_ml in users_ml:\n",
    "    compScore_ml[user_ml] = {}\n",
    "    user_X = dictionary[(user_ml)]\n",
    "    for compUser_ml in users_ml[user_ml]:\n",
    "        user_Y = dictionary[compUser_ml]\n",
    "        score = proxyLabelAssignement(user_X, user_Y)\n",
    "        if score >= 0.5:\n",
    "            compScore_ml[user_ml][compUser_ml] = 'Yes'\n",
    "        else:\n",
    "            compScore_ml[user_ml][compUser_ml] = 'No'          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Writing output to file: ', 'output/compatibility_with_proxyLabel.txt')\n",
      "('Writing output to file: ', 'output/compatibility_with_proxyLabel.txt')\n"
     ]
    }
   ],
   "source": [
    "print ('Writing output to file: ', out_filename)\n",
    "all_true = 0\n",
    "all_false = 0\n",
    "with open(out_filename, 'w+') as f:\n",
    "    for key in sorted(users):\n",
    "        f.write(str(key) + '\\t\\t:' + ' [')\n",
    "        for compUser in users[key]:\n",
    "            f.write(str(compUser) + ': ' + str(compScore[key][compUser]) + ', ')\n",
    "            if compScore[key][compUser] == 'Yes':\n",
    "                all_true += 1\n",
    "            else:\n",
    "                all_false += 1\n",
    "        f.write(']\\n')  \n",
    "\n",
    "\n",
    "print ('Writing output to file: ', out_filename)\n",
    "all_true_ml = 0\n",
    "all_false_ml = 0\n",
    "with open(out_filename_ml, 'w+') as f:\n",
    "    for key in sorted(users_ml):\n",
    "        f.write(str(key) + '\\t\\t:' + ' [')\n",
    "        for compUser in users_ml[key]:\n",
    "            f.write(str(compUser) + ': ' + str(compScore_ml[key][compUser]) + ', ')\n",
    "            if compScore_ml[key][compUser] == 'Yes':\n",
    "                all_true_ml += 1\n",
    "            else:\n",
    "                all_false_ml += 1\n",
    "        f.write(']\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of true results that were correct were:', 1610)\n",
      "('The number of false positives are:', 3390)\n",
      "('Total number of results: ', 5000)\n",
      "('Accuracy is without ML is: ', 0.309)\n",
      "('Accuracy is with ML is: ', 0.322)\n"
     ]
    }
   ],
   "source": [
    "accuracy = all_true/float(all_false+all_true)\n",
    "accuracy_ml = all_true_ml/float(all_false_ml+all_true_ml)\n",
    "print (\"The number of true results that were correct were:\", all_true_ml)\n",
    "print (\"The number of false positives are:\", all_false_ml)\n",
    "print (\"Total number of results: \", all_true_ml+all_false_ml)\n",
    "print (\"Accuracy is without ML is: \", accuracy)\n",
    "print (\"Accuracy is with ML is: \", accuracy_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** The accuracy without the ML learning is: 67.8%\n",
    "** The accuracy with ML is 69.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
