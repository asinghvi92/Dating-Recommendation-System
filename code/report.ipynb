{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dating/Friendship Recommender\n",
    "\n",
    "### By:\n",
    "- Saket Ati\n",
    "- Richa Surbhi\n",
    "- Akshit Singhvi\n",
    "- Kelly Luk Bounsawat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction and Problem Statement\n",
    "The project seeks to address how to recommend an individual to another individual based on their compatibility using attributes such as pets, age, gender, personal reflection, etc. The process would be interesting to work on due to our current societal norms. According to statistics, society does most of their communication through social media. People no longer communicate with their friends face to face and spend five minutes without looking down on their phones. As social media has become a large part in their lives, it takes over. People no longer met their significant others in bars, parks or big events. It has evolved from communication with friends to meeting people online. Apps such as Tinder, OkCupid and Coffee meets Bagel attempt to recommend individuals with others based on their profiles and common interests. Our team attempts to implement a recommendation system to understand how these popular sites attempt to match up couples. Trying to find the most efficient and accurate way to calculate the best match is not an easy feat as each individual’s idea of a perfect date vary due to their mood, generation and what’s currently on trend. Different companies have different approaches to the problem to account for a large variety of variables. Programming hard facts is simple, trying to program and understand an human’s preferences is a big challenge. The scope of our project is to implement a variation of a Reciprocal Recommender system. We are going to use the algorithm to calculate a compatibility score for a user-user pair. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Works\n",
    "Recommenders have been well researched to match users to other individuals, however there are not many that include  reciprocal systems. A Reciprocal system [3] recommends people to other people and a successful recommendation only occurs when both people reciprocate. This is exactly what Tinder does. Two people are matched up if and only if they both swipe-right for each other. \n",
    "\n",
    "Types of user profile data: There are two types of user profile data that can be used; implicit and explicit data. Implicit data is collected based on the user’s behavior on a platform. Example: how many times a user reaches out to other users vs. is contacted by other users. Some sort of implicit interpretation of the data is needed to use this data but sometimes if too many assumptions are made, it could lead to wrong results. Explicit profile data on the other hand is easier to collect and interpret since all the data is provided directly by the user in a survey or other forms.  We do not have implicit data available, so our project is simply using explicit data.\n",
    "\n",
    "Differentiation from existing work: Existing paper focuses on computing scores based on user’s own profile and his preference profile. That’s possible because the dataset contains the “message” information indicating whether one user has contacted any other user or not. However, we don’t have that attribute in our dataset, so we instead focus on identifying the features that can affect the compatibility scores. Further, we employ machine learning model on top of this to identify the “best” weights and then find the compatibility scores as the weighted sum of these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach/Methodology\n",
    "While reading the research paper [1], it was understood that the system could not be replicated exactly due to the lack of dataset. So instead we consider our current computational power, time and sanity. The following section is separated into four sections: a brief summary of the team’s approach to the problem is separated into four main steps, thorough detail explanation of each step along with their code, the main struggles we faced through the development of the recommendation system, and finally our interesting contributions.\n",
    "\n",
    "### Approach summary\n",
    "1. __Dataset__- Take data from OKCupid’s public database and split it into training and evaluation sets.\n",
    "\n",
    "2. __Identify features__- Implement the compatibility algorithm from table 2 from paper [1]. Since we don’t have the number of messages passed between users, we do an extra step to compensate for it. We segment each user profile into attributes based on his/her data. Then we identify different “features” to consider that are relevant to determine the compatibility score of 2 people. For eg, one of the features would be cosine scores of essay responses of users, another would be compatibility based on whether they like pets or not and so on. \n",
    "\n",
    "3. __Compatibility Score Generation__- Once we have identified the features, our aim is to weight them based on relevance using machine learning algorithms (logistic regression) and compute the final compatibility score. This is important because certain features might be more important than others and hence they must be weighted accordingly. We also use feature reduction techniques like SVD to reduce features and further explore latent features. For this step, we first randomly sample some users from the training set and then we manually assign binary compatibility scores to them. This is equivalent to providing ground truth training data to supervised machine learning model.\n",
    "\n",
    "4. __Evaluation__- With the calculated weights, we test the algorithm on testing data and evaluate the performance of our algorithm in terms of precision, recall and accuracy of the model w.r.t. the manual labelling of test data. \n",
    "\n",
    "![alt text](img/workflowDiagram.PNG \"Workflow Diagram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Approach\n",
    "The implementation is divided into five parts; Features, proxy labeling, machine learning, compatibility and calculating precision. Each of these parts communicate with each other to accomplish goals shown in figure 1. First, Compatibility parses the data set. Secondly, it then calls features to calculate the score for each attribute. Thirdly, it uses the values from machine learning configuration to calculate the end score on the testing data. Lastly, we calculate precision based on the results from compatibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "Initializing and deciding how each feature weighted/scaled is a key factor on how a user would be recommended to another. Based on the dataset found in OKCupid [2], a total of 28 of unique features were provided. Due to the large number of features, the team decided that not all 28 features are quite necessary for the purpose of displaying how effective reciprocal recommenders work. As the algorithm requires the system to do  recommendation calculations user by user, it was deemed wise to narrow down the number of features in exchange for time. For this reason, two changes were made to the dataset. First, the features were narrowed down to 3 features being used to quickly filter the compatibility between users and 18 features were used to calculate recommendations. Secondly, the possible answers returned by each feature were narrowed down. Some answers were redundant (ex. for drugs: no vs never) or appeared too specific to handle (ex. offsprings: has a kid, has kids, doesn’t have a kid, doesn't want kids; and, but might want them, wants them, doesn’t want any, doesn;t want more). Below is a short description of the decisions made whether to use the features or not:\n",
    "\n",
    "The below features were used as a way to distinguish each user from each other. Most users have provided responses for the below features. This provides a bit of personal information to determine if a user is similar/compatible with other users.\n",
    "\n",
    "__Possible responses for features considered:__\n",
    "\t- education \n",
    "        - high school/space camp, two-year college, university/masters program, law school/med school/Ph.D program\n",
    "    - religion\n",
    "        - atheist\n",
    "        - everything else\n",
    "\t- body type \n",
    "        - thin/skinny, average/fit, athletic/jacked, overweight/a little extra/curvy/full figured, used up\n",
    "\t- diet\n",
    "        - vegetarian/vegan, kosher, halal, anything/other\n",
    "\t- zodiac sign\n",
    "        - aquarius, pices, aries, Taurus, Gemini, cancer, leo, virgo, libra, scorpio, saggitarius and capricorn\n",
    "\t- age\n",
    "\t- smokes\n",
    "        - yes, sometimes/when drinking/trying to quit, no/never\n",
    "\t- drinks\n",
    "        - very often, often, socially, rarely, desperately, not at all\n",
    "    - essay\n",
    "        - 0: My self summary\n",
    "        - 1: What I’m doing with my life\n",
    "        - 2: I’m really good at...\n",
    "        - 3: The first thing people usually notice about me...\n",
    "        - 4: Favorite books, movies, show, music, and food\n",
    "        - 5: The six things I could never do without\n",
    "        - 6: I spend a lot of time thinking about...\n",
    "        - 7: On a typical Friday night I am...\n",
    "        - 8: The most private thing I am willing to admit...\n",
    "        - 9: You should message me if...\n",
    "\n",
    "__Feature used as hard filter:__\n",
    "\t- sex and orientation: Match gender preferences \n",
    "        - (ie. (m, f) and (straight, bi, gay))\n",
    "        \n",
    "__Features used for proxy labeling:__\n",
    "\t- pets: One way to determine is truly compatible\n",
    "    - zodiac sign: Match users with signs that are compatible with their own (ex. aquarius is compatible with leo and sagittarius)\n",
    "\n",
    "__Features Ignored:__\n",
    "\t- ethinicity: Majority users were the same ethnicity, so this did not make much of a difference\n",
    "\t- offspring: Too many options and was difficult to weigh in preferences if a user wishes for offsprings or not. \n",
    "\t- height: Most users did not provide this information\n",
    "\t- income and job: Wanted to make users to be matched without job/income bias, but rather for personality and character. In addition, most users did not provide their job/income description\n",
    "    - drug: Similar to alcohol and smoking feature\n",
    "    - speaks/language: all users spoke english.\n",
    "\n",
    "\n",
    "Once we determined what groups each features belong to, we defined how each quality is going to be weighted. As shown below, each feature was carefully considered and different ratings were generated to reflect the unique feature. \n",
    "\n",
    "__How each feature was scored:__\n",
    "\t- education: Users with relatively similar education levels receive a higher education score.\n",
    "\t- religion: Users with similar beliefs get higher weight, otherwise a low weight preference is given\n",
    "\t- body type: Users with similar physique get higher weight, however higher preference is given to users who are fit/athletic\n",
    "\t- diet: Users with similar dietary needs get better weights\n",
    "\t- age: Users within +/- 5 years. Smaller age gaps are given higher preference\n",
    "\t- smokes: Match users with similar smoking habit, however adding the condition where non-smokers have low tolerance of smokers\n",
    "\t- drinks: Match users with similar drinking habits\n",
    "    - essay: Using Cosine Similarity with stemming and casefolding to determine the distance between each other based on their responses\n",
    "    \n",
    "Given two unique user id's, a function is called to extract each feature information from both users and calculate the weights. Once all the calculations are completed for the two users, each value is then normalized so each feature does not weigh more than the others. The weight of each feature is calculated by the Machine Learning section of the code, which is seen in the section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Refer to features.py for the code for scoring each features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proxy Labeling\n",
    "Given a user x, our algorithm returns the top 5 users that have the highest compatibility score for user x in descending order. However, for the sake of checking its correctness, we use the metric of the pet compatibility and zodiac sign to verify if the recommendations that were outputted by the algorithm are relevant. For example, if user A likes dog and user B likes cat, they are considered to be more compatible than the user who doesn’t like dogs and the user who likes dog. Proxy labeling does exactly this. Given two users, it returns a compatibility value based on their likes/dislikes of pets and probability a zodiac sign liking another. We used this metric to measure correctness because our dataset lacked any labeling information. As recommender systems rely on supervised learning, we are required to use labels to train our data. An alternative method to verify if the algorithm is computing the compatibility score correctly would be to process the data manually. In other words, we would manually check if two users are compatible by analyzing every factor and confirm if they should be compatible or not. We chose not to do the manual option due to the lack of time and possible bias. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "As described in previous section, we use “pets preference” and “Zodiac sign” attributes to compute a proxy label that represents compatibility scores between different users. With help of these known labels, we can now utilize the training dataset by formulating a supervised machine learning problem and “learn” the weights of features for computing final compatibility score. \n",
    "Assuming that we have ‘n’ users in training dataset. We can now generate a proxy label for each user- user pair and each pair would constitute a training data point. Mathematically, it can be formulated as-\n",
    "\n",
    "__Given__: \n",
    "\n",
    "    Training dataset {Xi ,yi}\n",
    "    Where, i= 1 to n*n \n",
    "\t\tXi = f(UserX, UserY) = feature vector of UserX and UserY.        \n",
    "        yi = ProxyLabel (UserX, UserY) = Compatibility Score of UserX and UserY \n",
    "\n",
    "__Aim__: To learn the weights of features from training set and determine compatibility scores for test data. Mathematically, we construct a function of features that generates the compatibility scores for test data as- \n",
    "\n",
    "              Ψ: Xi ->yi  \n",
    "\t\t\twhere, Ψ: function learnt from training \n",
    "\t\t\tXi =Test Feature Vector of UserX and UserY\n",
    "\t\t\tyi= Compatibility Score of UserX and UserY\n",
    " \n",
    "__Algorithm__: We tried a number of algorithms for this problem. For each approach, we experimented with different sized training and testing datasets. The results are compared and discussed in next section. The details of the implemented approaches are as follows- \n",
    "\n",
    "A) __Naive Method__ - In this method, we assumed all features to be equally important. So after generating the features for each user-user pair in evaluation set, we simply average those values to compute the compatibility score. \n",
    "Such a method needs no training and thus, has the obvious advantage in terms of computation speed. However, this fails to scale up as the size of data increases. This was merely used to serve as a baseline for machine learning methods.\n",
    "\n",
    "B) __Logistic Regression (Classification Machine Learning Algorithm)__ - In lieu of simple averaging of all features, we employed machine learning algorithm that learns to rank by estimating weights for each feature. Given the huge amount of available dataset (19,000 users=> 19,000*19,000 data points) and binary classification problem (match or not match), we used simple logistic regression algorithm to train the classifier. We implemented it using scikit-learn python package, details of which are available in the attached code. \n",
    "\n",
    "C) __Linear Regression (Regression Machine Learning Algorithm)__ - Though the task is clearly classification problem (match or not match), it suffers from a serious drawback. As the evaluation set is huge (large number of user- user pairs) and there are only 2 classes (match and not match), we have a large number of recommendations for each user. Furthermore, we don’t have a way to pick the top “k” recommendations as we have used classification algorithm. This motivated us to use regression algorithm instead of classification. We explored linear regression algorithm in this approach. Linear regression is similar to logistic regression in the sense that it models the data by fitting a linear decision line. However, unlike logistic regression, the output is a continuous real values number  instead of a binary value. Implementation was done using scikit-learn python package and details are available in attached code. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples num 50\n",
      "Running Logistic Regression\n",
      "training successfully completed, myclassifier.log generated !!!\n",
      "('time taken for training: ', 16.171000003814697)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# To run training\n",
    "\n",
    "from MLtrainingtesting import run_training\n",
    "## This step can be skipped because it takes a long time to run for higher sample sizes.\n",
    "## We have saved an existing training output in code/myClassifier.log.\n",
    "## However, git is known to cause issues when reading files using cPickle. \n",
    "## If the next step fails, then come back and run training for a small number like 50.\n",
    "\n",
    "run_training(50)\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compatibility Scores\n",
    "The compatibility_score.py uses the features.py and the machine learning part to actually compute the scores. calculate_compatibility() function accepts one parameter *max_users*, that is the number of users that we want to calculate scores for. It extracts *max_users* number users from the evaluation set and creates two matrices of size *max_users *x* max_users*. These matrices hold the scores for a (user, user) pair. One matrix holds the scores computed without machine learning and the second for scores with machine learning (weights learnt by the training). The default value for (user X, user X) is set to -1 so this is ignored when fetching the top-*k* users. The default for (user X, user Y) is set to 0. Since ours is a Reciprocal Recommender, we made the assumption that the scores for (user X, user Y) is equal to (user Y, user X). This also reduces the number of computation that we have to do to (n)(n-½) per matrix. \n",
    "For each (user X, user Y) pair, this script first checks if the sex and orientation of the user pair matches. For matching user pairs, the script gets the scores with and without ML using the functions described above and fills the score matrices. Once all the scores are computed, the script iterates through each user and gets the users with top-k compatibility scores and prints this to a file (compatibility.json and compatibility_ml.json). We set the value of k to 5. The output of this step are two files that contain 5 recommendations for each user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done reading users file. Number of users found =  12748 !\n",
      "Extracting 100  random users...\n",
      "Created and initialized Score matrix of size  100 x 100\n",
      "Computing Compatibility Scores...\n",
      "Done computing scores!\n",
      "Extracting top  10  users\n",
      "Writing output to file:  <open file 'output/compatibility.json', mode 'w' at 0x0000000031379E40>\n",
      "Writing ML output to file:  <open file 'output/compatibility_ml.json', mode 'w' at 0x0000000031379ED0>\n",
      "Done computing compatibility scores! Total time:  18.8760001659\n"
     ]
    }
   ],
   "source": [
    "# To run the compatibility score algorithm\n",
    "\n",
    "from compatibility_score import calculate_compatibility\n",
    "\n",
    "calculate_compatibility(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Precision:\n",
    "Precision is calculated using the results from Compatibility and comparing its results with proxy labeling. All five results that were computed from Compatibility algorithm were checked against the proxy labeling. So in terms of precision, the code calculates the number of results for each user which are checked for correctness using proxy labeling. After that, all the true positives are divided by true positives and false positives. In this way, we are able to see the correctness of the Compatibility algorithm against the proxy labeling as the standard. The reason we decided to use precision as the metric is because we can only easily calculate both false positives and true positive scores. The compatibility algorithm returns a set of five possible mates per user. So we use these values to compare it against the proxy label which gives us the actual number for true positives. Therefore calculating precision in this form. In order to measure accuracy in the traditional sense (ie. tp/(tp+tn+fp+fn)), we would need false negatives and true negatives which we don’t have in our scenario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing output to file:  output/compatibility_with_proxyLabel.txt\n",
      "Writing output to file:  output/compatibility_with_proxyLabel_ML.txt\n",
      "Precision is without ML is:  0.676\n",
      "Precision is with ML is:  0.748\n"
     ]
    }
   ],
   "source": [
    "# To calculate precision\n",
    "## Be sure to run the above calculate_compatibility() function before running this!!\n",
    "from verifyCompScore import verify_compatibility\n",
    "\n",
    "verify_compatibility()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check\n",
    "We saved our responses to the dataset questions in the file dataset/users_us.json. We took our responses and injected them into the evaluation dataset before running the algorithm. This was used to manually look at the users that we matched up with. We built a simple web app that will allow us to view the profiles of two users. Unfortunately the files that our web app is using have to be manually copied to the hosting site so it won't see the latest output generated above. The web app is currently using output from a previous run. \n",
    "\n",
    "Link to the app: http://people.tamu.edu/~sati/datingRecommender/index.html\n",
    "\n",
    "Wait till you read the message that says \"Ready to compare\".\n",
    "You can put in one of our ID's (listed below) in the \"Your User Id\" textbox and click compare. This will list the ID's of our recommendations on the top right. Now take one of those IDs and put it in the second textbox and click compare again. Now you'll see the profiles of both the users.\n",
    "\n",
    "Our user IDs:\n",
    "Richa - 64323\n",
    "Kelly - 64324\n",
    "Akshit - 64325\n",
    "Saket - 64326\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Struggles\n",
    "There were two main issues we ran into; training data size and the lack of labeled data. \n",
    "\n",
    "- Of the 27,000 user profiles that we had, a whole lot of them had junk/unusable data. So first we cut our dataset to 15,000 users. Of the remaining users, all the essay questions contained html tags and newline characters. So we had to cleaning up the data for it to be usable.   \n",
    "- Training data caused issues because the large amount of data we got couldn’t be used to train the model. The dataset provided 15,000 users, each with unique features. Ideally, we would used 70% of the dataset for training and 30% for testing set. This comes out to approximately 10,000 users by 10,000 users calculations. It is impossible to compute the training set of that much data and build a model due to the limitation of our machine power, time and sanity. If given more time, we can refine our model to be more accurate by having a higher training set. \n",
    "- Another setback was to figure out how to verify the correctness of the Compatibility algorithm. Since the dataset didn’t provide any labels with recommended user id’s. Ideally we would create the correctness of the profiles based on doing manually labeling of various different users. In other words, generate Ground Truth. However, due to the limited time, we decided to go with Proxy labeling which is defined above. We did however, create user profiles for each of our team members and observed the results as a sanity check. The below function adds our user profiles and computes the compatibility. We built a simple html/angularjs app to put our user id's in and compare with the users that we were recommended. \n",
    "\n",
    "### Interesting Contributions\n",
    "Three most interesting contributions done on our project design and/or implementation are:\n",
    "1. Reduced the recommendation problem to machine learning problem - The original problem was to recommend users to each user based on their profile similarities. We, instead, create user-user pairs and output a compatibility score for each of these pair. Based on this score, we then pick the top-k recommended results for that user. This is interesting approach as it opens the wide range of machine learning algorithms that can now be in applied in context of recommender systems. \n",
    "2. Combined objective and subjective features - For each person’s profile, the dataset provided both objective answers data (body type, age, etc.) and subjective data (response to 10 standard essay type questions). We utilized both sets of features and normalized them between 0 and 1 to combine under 1 table. \n",
    "3. Calculating Precision - Common Sense would dictate we would account for success based on accuracy (ie. number of successful matches/total number of possible matches). However, with the nature of how we implemented our system this was not possible. With a large 15,000 unique users, it would computational difficult to compute the probability of 15,000 by 15,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluation/Results\n",
    "To evaluate the results, we have used Precision measure. For each user, we take the top k results returned by the algorithm (We experimented with different values of k and find best results with 3). Then we check these returned results against the pre- computed proxy label and calculate the precision score. Calculating such metric make sense because in a typical recommender system, we have a large pool of potential candidates for each profile. Thus, we don’t care if the algorithm miss a few possible matches (low recall) but all the ones returned should be with high confidence (high precision). \n",
    "\n",
    "Following table summarizes the results obtained by various algorithms. We have plotted the precision of each algorithm against different sized training dataset and also compared different algorithms against each other. The discussion is presented in next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/res.JPG \"res\")\n",
    "![alt text](img/featureAvg.PNG \"Feature Average\")\n",
    "![alt text](img/linearReg.PNG \"Linear Regression\")\n",
    "![alt text](img/logisticReg.PNG \"Logistic Regression\")\n",
    "![alt text](img/precisionCom.PNG \"Precision Comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Future Works\n",
    "The results and graphs obtained are presented. Doing this project, we learnt how to develop a reciprocal recommendation systems. We analyzed the given dataset and extracted the features for each user profile. Then we explored machine learning algorithms to learn to rank these features and generated a compatibility score. In the process, we referred the research papers and learnt to implement machine learning algorithms in context of recommendation systems. \n",
    " We had expected machine learning algorithms to outperform the simple averaging and the precision for each approach to increase with increase in training data set size. However, the observations are not consistent with the premise. We believe this issue is due to non- availability of ground truth data. The proxy label that we created seems inaccurate and hence the results are not sufficient to draw a fair conclusion. Furthermore, the huge computation time made it difficult to run simulations for bigger datasets and complicated machine learning algorithms. For future work, we would like to try following approaches:\n",
    "- Either get labelled dataset or do manual labelling for this rather than using proxy label. One idea could be to use crowdsourcing for this purpose. \n",
    "- For matching the essay responses of 2 users, use sophisticated measures like sentence2vec rather than simple cosine score\n",
    "- Run complex machine learning algorithms like SVM and Random Forest on more powerful server machines \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "- Link to original paper 'RECON: a reciprocal recommender for online dating'\n",
    "http://www.cs.usyd.edu.au/~judy/Homec/Pubs/2010_RecSysUSYD.pdf\n",
    "- Link to dataset OKCupid: \n",
    "https://github.com/rudeboybert/JSE_OkCupid\n",
    "- Link to our repository:\n",
    "https://github.tamu.edu/kelly-12azn/datingRecommender\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
